{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: Stylianos Kampakis & Shreesha Jagadeesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the full solution for Project 1 until the end of Milestone 4 for Novelty Detection using Isolation Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a> <br>\n",
    "## Table of  Contents\n",
    "1. [Introduction](#1)\n",
    "\n",
    "1. [Initialization](#2)\n",
    "    1. [Load packages](#21)\n",
    "    1. [Define Metadata](#22)\n",
    "    \n",
    "1. [Load Data](#3)\n",
    "\n",
    "1. [Data Insights](#4)\n",
    "    1. [Data Structure](#41)\n",
    "    1. [Summary Stats](#42)\n",
    "    1. [Unique Value Checking](#43)\n",
    "    1. [Identifying 'Bad Columns'](#44)\n",
    "\n",
    "1. [Data Cleansing](#5)\n",
    "    1. [Data Reduction](#51)\n",
    "        1. [Dropping Bad Columns](#511)\n",
    "        1. [Null Value Removal](#512)\n",
    "        1. [Data Encoding](#513)\n",
    "    1. [Export csv file for later use](#52)\n",
    "\n",
    "1. [Modelling Workflow](#6)\n",
    "    1. [Data Prep](#61)\n",
    "        1. [Feature Target Split](#611)\n",
    "        1. [Train-Test Split](#612)\n",
    "        1. [Normalizing Numerical Variables](#613)\n",
    "    1. [Estimate of Baseline Accuracy - Class Distributions](#62)\n",
    "    1. [Semisupervised & Unsupervised Techniques for Novelty & Outlier Detection](#63)\n",
    "        1. [OneClassSVMs for Novelty Detection](#631)\n",
    "        1. [Robust Covariance for Outlier Detection](#632)\n",
    "        1. [Isolation Forest for Novelty Detection](#633)\n",
    "        1. [Local Outlier Factor for Novelty Detection](#634)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='1'>Introduction</a>  \n",
    "\n",
    "As described on the project page, the dataset contains the Thyroid disease data that is imbalanced. Before running this notebook, please make sure that you have gone through the first project in the LiveSeries and the starter template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='2'>Initialization</a>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='21'>Load Packages</a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the minimum number of packages to get started and add more as we go along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy\n",
    "from scipy.io.arff import loadarff\n",
    "import scipy.io as sio\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report,confusion_matrix \n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='22'>Define Metadata</a>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of the target class column here instead of manually typing it out everywhere\n",
    "target_class_name = 6\n",
    "\n",
    "# Fill in the names of what you want to call the 0 and 1 class\n",
    "labels = ['inliers', 'outliers']\n",
    "\n",
    "# The following file is downloaded from http://odds.cs.stonybrook.edu/thyroid-disease-dataset/ and kept in the data/raw folder\n",
    "input_file_name = 'thyroid.mat'\n",
    "\n",
    "# Using relative path path to specify that the data subfolder is two directories up from the current folder \n",
    "raw_data_folder = '../../01-Data/Raw/'\n",
    "processed_data_folder = '../../01-Data/Processed/'\n",
    "\n",
    "# Any exported artifacts will have this date\n",
    "export_date = '20211101'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='3'>Load Data</a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will load the data and perform some necessary preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../01-Data/Raw/thyroid.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/media/brendon/Ext4FastData/bin/miniconda3/envs/Python/lib/python3.10/site-packages/scipy/io/matlab/_mio.py:39\u001b[0m, in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Probably \"not found\"\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../01-Data/Raw/thyroid.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data\u001b[38;5;241m=\u001b[39m\u001b[43msio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadmat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_data_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_file_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m data\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# This is an interesting format with the metadata available as header, version and globals while the features are given \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# in the X array and the y \u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# There appears to be 6 attributes in X, all numerical \u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# The y values seems to be already encoded as numbers too\u001b[39;00m\n",
      "File \u001b[0;32m/media/brendon/Ext4FastData/bin/miniconda3/envs/Python/lib/python3.10/site-packages/scipy/io/matlab/_mio.py:225\u001b[0m, in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;124;03mLoad MATLAB file.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m    3.14159265+3.14159265j])\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    224\u001b[0m variable_names \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariable_names\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 225\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_context(file_name, appendmat) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    226\u001b[0m     MR, _ \u001b[38;5;241m=\u001b[39m mat_reader_factory(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    227\u001b[0m     matfile_dict \u001b[38;5;241m=\u001b[39m MR\u001b[38;5;241m.\u001b[39mget_variables(variable_names)\n",
      "File \u001b[0;32m/media/brendon/Ext4FastData/bin/miniconda3/envs/Python/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/media/brendon/Ext4FastData/bin/miniconda3/envs/Python/lib/python3.10/site-packages/scipy/io/matlab/_mio.py:17\u001b[0m, in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_context\u001b[39m(file_like, appendmat, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     f, opened \u001b[38;5;241m=\u001b[39m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappendmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "File \u001b[0;32m/media/brendon/Ext4FastData/bin/miniconda3/envs/Python/lib/python3.10/site-packages/scipy/io/matlab/_mio.py:45\u001b[0m, in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m appendmat \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_like\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     44\u001b[0m         file_like \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mat\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReader needs file name or open file-like object\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     49\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../01-Data/Raw/thyroid.mat'"
     ]
    }
   ],
   "source": [
    "data=sio.loadmat(raw_data_folder + input_file_name)\n",
    "\n",
    "data\n",
    "# This is an interesting format with the metadata available as header, version and globals while the features are given \n",
    "# in the X array and the y \n",
    "\n",
    "# There appears to be 6 attributes in X, all numerical \n",
    "# The y values seems to be already encoded as numbers too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the features and target objects in their own variables for easy retreival\n",
    "X=data['X']\n",
    "y=data['y']\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a dataframe\n",
    "df=pd.DataFrame((np.concatenate((X, y), axis=1)))\n",
    "\n",
    "# Take a random sample to see how the data looks like\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the head & tail to make sure there is nothing going on at the last row or the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No trouble with loading the data. Both the head and tail are clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=4 > Data Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='41'>Data Structure</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see the data structure\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the columns have null values at first glance, but we will run a more thorough diagnostic later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='42'>Summary Stats</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check out each column's summary statistics\n",
    "Note that only the numerical columns will be described\n",
    "Also you will want to exclude the discrete columns whose summary stats will give non-sensical values like 'customer_id' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n",
    "# Looks like all the numbers are between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='43'>Unique Value Checking</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    print(column, len(df[column].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the columns have atleast 2 unique values, hence there is less of a chance of quasi-constant values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='44'>Identifying Bad Columns</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bad_columns_function(dataframe):\n",
    "    '''\n",
    "    Args: dataframe for which there maybe columns of concern that need to be fixed or deleted\n",
    "    \n",
    "    Logic: Find the columns that have \n",
    "    Null values\n",
    "    blanks in the strings\n",
    "    quasi constant/constant values defined by less than 1% variance\n",
    "    \n",
    "    Returns: 4 lists containing those features that have nulls, blanks, constant values throughout for numerical and categorical\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ###### Finding Null Values\n",
    "    null_col_list = dataframe.columns[dataframe.isna().any()].tolist()\n",
    "    \n",
    "    print('Identified {} features with atleast one null'.format(\n",
    "        len(null_col_list)))\n",
    "\n",
    "    ###### Finding Blank Spaces in the object column\n",
    "    # Non-obvious nulls such as blanks: The line items where there are spaces \n",
    "    blank_space_col_list = []\n",
    "    object_columns = dataframe.select_dtypes(include=['object']).columns\n",
    "\n",
    "    for col in object_columns:\n",
    "        if sum(dataframe[col]==' '):\n",
    "            blank_space_col_list.append(col)\n",
    "\n",
    "    print('Identified {} features with atleast one blank space'.format(\n",
    "        len(blank_space_col_list)))\n",
    "    \n",
    "    ####### Finding Quasi Constant/Constant Value in numerical columns\n",
    "    # Lets remove the variables that have more than 99% of their values as the same \n",
    "    # ie their standard deviation is less than 1 %\n",
    "    \n",
    "    numeric_df = dataframe._get_numeric_data()\n",
    "    constant_numeric_col_list = [col for col in numeric_df.columns if numeric_df[col].std()<0.01]\n",
    "\n",
    "    print('Identified {} numeric features that have quasi-constant values'.format(\n",
    "        len(constant_numeric_col_list)))\n",
    "    \n",
    "    # We use a separate logic for the non-numerical variables because if you have closely varying float values\n",
    "    # then the below code snippet wont pick it up\n",
    "    \n",
    "    ###### Finding Quasi Constant/Constant non_numeric value\n",
    "    constant_non_numeric_col_list = []\n",
    "    \n",
    "    # Find the columns that are not in numeric_df\n",
    "    non_numeric_col_set = set(dataframe.columns) - set(numeric_df.columns)   \n",
    "\n",
    "    for col in non_numeric_col_set:\n",
    "        categorical_mode_value = (dataframe[col].mode().values)[0]\n",
    "        fractional_presence = sum(dataframe[col]==categorical_mode_value)/len(dataframe) \n",
    "    \n",
    "        if fractional_presence > 0.99:\n",
    "            constant_non_numeric_col_list.append(col)\n",
    "            \n",
    "    print('Identified {} non-numeric features that have quasi-constant values'.format(\n",
    "        len(constant_non_numeric_col_list)))\n",
    "    \n",
    "    return null_col_list, blank_space_col_list, constant_numeric_col_list, constant_non_numeric_col_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the above custom function to figure out the if there are any columns we need to be concerned about\n",
    "null_col_list, blank_space_col_list, constant_numeric_col_list, \\\n",
    "constant_non_numeric_col_list = find_bad_columns_function(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thankfully, there is no need to worry about any of the columns in this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='5'>Data Cleansing</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='51'>Data Reduction</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='511'>Dropping Bad Columns</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this because there are no columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='512'>Null Value Removal</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No null values to drop from the rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='513'>Data Encoding</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are no categorical variables to encode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='52'>Export cleaned csv file for later use</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a backup copy of the processed dataframe \n",
    "# so that you don't need to run the entire notebook again just to get to the next stage\n",
    "\n",
    "cleaned_dataframe_name='cleaned_thryoid_dataframe'\n",
    "df.to_csv(processed_data_folder + cleaned_dataframe_name + export_date + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 1 Ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 2 Begins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id = 6 > Modelling Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id = 61 > Data Prep "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='611'>Feature - Target Split</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(target_class_name, axis=1)\n",
    "y = df[target_class_name]\n",
    "\n",
    "print(X.shape )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='612'>Train-Test Split</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test set and \n",
    "# make sure that the stratification is done so that roughly proportional instances of the minority class is present in the train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# These are the original dimensions and the outlier class occurrences \n",
    "print(X_train.shape, X_test.shape, sum(y_train==1), sum(y_test==1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id = 613 > Normalizing numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not needed here because the variables are already scaled between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='62'>Estimate of baseline accuracy - Class Distributions </a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out the class distribution percentage and round it to 3 decimal places\n",
    "\n",
    "print('Percentage of inliers is {} %'.format(\n",
    "    round(df[target_class_name].value_counts()[0]/len(df) * 100,3)))\n",
    "\n",
    "print('Percentage of outliers is {} %'.format(\n",
    "    round(df[target_class_name].value_counts()[1]/len(df) * 100,3)))\n",
    "\n",
    "# A dumb model that predicts everything as being 0, will generate a baseline accuracy of 97.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "pd.value_counts(df[target_class_name]).plot.bar()\n",
    "plt.title('Histogram of class distributions')\n",
    "plt.xlabel(labels[1])\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylabel('Frequency')\n",
    "df[target_class_name].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline accuracy to beat is 97.5% if you predict everything as being 0 (i.e. the majority class)\n",
    "\n",
    "What about the other metrics like Precision, Recall and F1 score? They would all be 0% because there will be no True Positives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the number of outliers that we observe in the overall dataset. \n",
    "outliers_fraction=0.025\n",
    "\n",
    "#We will use a multiple of this as the nu or contamination factor to tell the model how many to expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='63'> Scikit Learn Algorithms for Novelty & Outlier Detection </a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q. Why do we need specialized techniques for Anomaly Detection rather than just use binary classification algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to binary classification problems, the minority class, i.e. the anomalies, are quite rare and often different from each other. So the usual binary classification algorithms fail to 'learn' a representation of the minority class. We will attempt to use binary classification methdods with resampling techniques that balance out the distributions of the 2 classes in projects 2 & 3 but in this Project we will focus on out-of-the-box Anomaly detection algorithms provided by Scikit Learn\n",
    "\n",
    "The previous plot of class distributions gave an indication of how rare our samples are and this warrants these special techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference between Novelty, Outliers and Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many use cases, the differences between them are irrelevant. As long as the sample is 'sufficiently' different, we can call them whatever we want. However there are specialized use cases where huge performance gains can be achieved by using a semi-supervised technique of Novelty Detection vs completely unsupervised Outlier Detection. Hence it is important to understand what the differences are\n",
    "\n",
    "Novelty Detection: The training data is not polluted by outliers and we are interested in detecting whether a new observation is an outlier. In this context an outlier is also called a novelty. This would mean that our input train data during model fitting cannot contain the minority class. \n",
    "\n",
    "Outlier Detection: The training data contains outliers which are defined as observations that are far from the others. Outlier detection estimators thus try to fit the regions where the training data is the most concentrated, ignoring the deviant observations.\n",
    "\n",
    "Both Novelties and Outlier samples are referred to as the Anomalies\n",
    "\n",
    "Further details are in the sklearn documentation page\n",
    "https://scikit-learn.org/stable/modules/outlier_detection.html#outlier-detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following milestones, we will explore applying both types of algorithms on our training dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='631'>OneClassSVMs for Novelty Detection</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to semi-supervised approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem solved by OneClassSVM is for novelty detection using an semi-supervised approach\n",
    "\n",
    "As Leo Tolstoy said ‘All happy families are alike; each unhappy family is unhappy in its own way.’\n",
    "The implication for our use case is that OneClassSVM excels in situations where the inliers are all alike but the outliers are all special snowflakes. Datasets that have such as issue prevents the use of traditional 2-class binary classification problems and instead we use a 'unary' classification problem. Only the statistics of normal operation are known from the inliers\n",
    "\n",
    "*We will be using OneClassSVM in a similar manner as unsupervised clustering. The approach processes the data as a static\n",
    "distribution, pinpoints the most remote points, and flags them as potential outliers.* \n",
    "\n",
    "**The main assumption on the data is that the outliers are actually separated from the inliers and hence the OCSVM algorithm can indeed separate it out.** \n",
    "\n",
    "The main thing to note is that *deleting the minority class from the input train data improves the performance when the model later sees the outliers mixed in* with normal inliers. This would be a case of semi-supervised because we are using the labels but not necessarily to fit the model but to remove the minority class samples to improve the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://proceedings.neurips.cc/paper/1999/file/8725fb777f25776ffa9076e44fcfd776-Paper.pdf\n",
    "\n",
    "https://en.wikipedia.org/wiki/One-class_classification\n",
    "\n",
    "https://eprints.whiterose.ac.uk/767/1/hodgevj4.pdf\n",
    "\n",
    "https://www.datatechnotes.com/2020/04/anomaly-detection-with-one-class-svm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function to hold the binary classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the same function from the starter template\n",
    "def custom_classification_metrics_function(X_test, y_test, labels, classifier):\n",
    "    '''\n",
    "    Args: The features and the target column; the labels are the categories, sklearn classifier object\n",
    "    Calculates Classification metrics of interest\n",
    "    Returns: A dictionary containing the classification metrics\n",
    "    '''\n",
    "\n",
    "    # Generate the predictions on the test data which forms the basis of the evaluation metrics\n",
    "    test_pred = classifier.predict(X_test)\n",
    "        \n",
    "    # This is to convert the output labels so that the confusion matrix and the accuracy scores work correctly\n",
    "    # The anomalies identified by sklearn are labelled as -1 but we want them to recognized as 1, so we flip the labels\n",
    "    test_pred = test_pred*-1\n",
    "    test_pred[test_pred==-1]=0\n",
    "    \n",
    "    ### Classification report\n",
    "    print(classification_report(y_test, test_pred, target_names=labels))\n",
    "\n",
    "    ### Probability scores\n",
    "    \n",
    "    # Unlike the usual classifiers, Anomaly detection algorithms don't have a predict_proba() method that can be applied\n",
    "    # Instead we have to get the raw scores from the decision function applied on the test data\n",
    "    decision_score_list = classifier.decision_function(X_test)\n",
    "    \n",
    "    # ......and turn them into numbers between 0 and 1 which can be treated as probabilities\n",
    "    scaled_decision_score_list = MinMaxScaler().fit_transform(decision_score_list.reshape(-1, 1))\n",
    "\n",
    "    # This is just to flatten the list and subtract the no.s so that the outliers are closer to 1 rather than 0\n",
    "    y_scores = [1-item for sublist in scaled_decision_score_list for item in sublist]\n",
    "\n",
    "\n",
    "    ### Confusion Matrix\n",
    "    confusion_matrix_test_object = confusion_matrix(y_test, test_pred)\n",
    "                \n",
    "    # Initialize a dictionary to store the metrics we are interested in\n",
    "    metrics_dict = Counter()\n",
    "\n",
    "    # These are all the basic threshold-dependent metrics\n",
    "    metrics_dict['Accuracy']  = float(\"{0:.4f}\".format(accuracy_score(y_test, test_pred)))\n",
    "\n",
    "    # The following are more useful than the accuracy\n",
    "    metrics_dict['Precision'] = float(\"{0:.4f}\".format(precision_score(y_test, test_pred, average='macro')))\n",
    "    metrics_dict['Recall'] = float(\"{0:.4f}\".format(recall_score(y_test, test_pred, average='macro')))\n",
    "    metrics_dict['F1'] = float(\"{0:.4f}\".format(f1_score(y_test, test_pred, average='macro')))\n",
    "\n",
    "    metrics_dict['TN'] = confusion_matrix_test_object[0][0]\n",
    "    metrics_dict['TP'] = confusion_matrix_test_object[1][1]\n",
    "    metrics_dict['FN'] = confusion_matrix_test_object[1][0]\n",
    "    metrics_dict['FP'] = confusion_matrix_test_object[0][1]\n",
    "\n",
    "    # These two are threshold-invariant metrics\n",
    "    metrics_dict['ROC AUC'] = float(\"{0:.4f}\".format(roc_auc_score(y_test, y_scores)))\n",
    "    metrics_dict['Average_Precision'] = float(\"{0:.4f}\".format(\n",
    "                                        average_precision_score(y_test, y_scores, average='macro', sample_weight=None)))\n",
    "\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prep train data for one class novelty detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate X_train with y_train. Then filter out the samples that we know are outliers\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "inlier_X_train = df_train[df_train[target_class_name]==0].drop(target_class_name, axis=1)\n",
    "\n",
    "# We will use inlier_X_train as the input dataset\n",
    "inlier_X_train.shape\n",
    "# That makes sense to have 3017 - 74= 2943 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize dictionaries to store the instantiated models and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 3 SVM models with different parameters to get a sense of the performance across the metrics of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the variants of the model with different kernel types\n",
    "# Add a prefix Novelty' to indicate that our training method relies on a pure sample of inliers\n",
    "classifier_dict = {\"Novelty OCSVM RBF\":OneClassSVM(nu=outliers_fraction*4, kernel=\"rbf\"), \n",
    "                   \"Novelty OCSVM poly degree 2\":OneClassSVM(nu=outliers_fraction*4, kernel=\"poly\", degree=2),\n",
    "                   \"Novelty OCSVM poly degree 3\":OneClassSVM(nu=outliers_fraction*4, kernel=\"poly\",degree=3)}\n",
    "\n",
    "# nu is a hyperparameter in the model that controls the upper bound of the training error. \n",
    "# nu is usually set empirically (can also be tuned by hyperparameter tunning).\n",
    "\n",
    "# Note: though the actual no. of outliers in the train data is 2.5%, due to the inherent inaccuracies, \n",
    "# it makes sense to intentionally tell the model to look for more outliers than is the case to increase Precision\n",
    "# This of course increases the False Positives but that is a tradeoff worth making when detection of rare diseases is needed\n",
    "\n",
    "# Initialize a dataframe with the columns that we want to store being the various metrics of interest\n",
    "metrics_df = pd.DataFrame(\n",
    "columns = ['Accuracy','Precision','Recall','F1', 'ROC AUC', \n",
    "           'FN','TP','FP','TN', 'Average_Precision'],\n",
    "index1 = [classifier_name for classifier_name in classifier_dict.keys()])\n",
    "\n",
    "metrics_df\n",
    "# As seen below, the rows are the names of the classifiers and the columns are the metrics of interest\n",
    "# These NaNs will be replaced with the values of the metrics when you train the model and predict on the test data next\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note on evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scientists working on skewed data problems need to keep in mind a) Defining & capturing the right metrics and b) Interpreting metrics\n",
    "\n",
    "Defining & capturing the right metrics: In the custom classification metrics function, we have intentionally captured the macro averaged metrics for the Precision, Recall, F1, Average Precision scores. Specifying the macro version ensures that sklearn corrects for the skewed data and weights both classes equally. \n",
    "\n",
    "Interpreting metrics: The correct interpretation relies on the use case and there is no one size fits all. In the case of outlier detection like Thyroid detection, we would typically want as much of the positive instances (in this case the outlier class) to be captured at the expense of having more False Positives \n",
    "This is because even if there are False Positives, a detailed screening can clear them out, but if a patient's case is not detected at all (i.e. False Negatives), then they are sent home and lost to follow-up evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the various classifiers and store the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier_name, classifier in classifier_dict.items(): \n",
    "    \n",
    "    # Replace the X_train with inlier_X_train for one-class novelty training\n",
    "    classifier.fit(inlier_X_train)\n",
    "    print('***********') \n",
    "    print(classifier_name) \n",
    "    metrics_dict = custom_classification_metrics_function(X_test, y_test, labels, classifier)\n",
    "    print(metrics_dict)\n",
    "    \n",
    "    # store the metrics as a single row in the dataframe against each classifier name\n",
    "    metrics_df.loc[classifier_name] = metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe the performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df\n",
    "# Note that the Precision, Recall, F1 score are macro averaged metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our use case in detecting thyroid anomalies, we would want a model that has a relatively high Recall to not miss out on  outliers (i.e. fewer False Negatives). The OCSVM with RBF kernel has the best Recall while being reasonable with the no. of False Positives relative to the polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_metrics_df = metrics_df.copy()\n",
    "cumulative_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 2 Ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 3 Begins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='632'> Robust Covariance for Outlier Detection </a> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
